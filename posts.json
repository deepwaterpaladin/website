[
    {
        "id":"statscanpy",
        "title": "Developing StatsCanPy",
        "author": "Rielly H. Young",
        "date": "August 19th, 2024",
        "content": [
            "Desipte it's flaws, python holds a special place in my heart. While most of my professional development utilizes C#, there are still opertunities where python is my go-to.",
            "My favourite aspect of python is how quickly you can get to coding. When I'm building a pipeline or trying to do some adhoc analysis, all you need is a notebook & you can start writing.",
            "I was recently in a position where I needed to do some ahdoc analysis using StatsCan data. To do this, I needed to built a small ETL pipeline in a Databricks environment to process ~5GBs of data.",
            "The initial problem was simple: StatsCan's API required the use of a table-id when querying data. The popular python module * statscan * acts as a wrapper for the API and returns data as pandas dataframes. When using module, the table id needed to be cleaned, which was annoying (i.e., removal of hyphens & dropping the last two numbers). Thus, users (myself in this case) needed to search for the table-id that they wanted, then keep track of which IDs are associated with each table. Another (small) issue was the pandas aspect; I was working in Databrick & therefore wanted to leverage PySpark.",
            "So with these two issues in mind, I went to work building my own module (with the very original name <StatsCanPy>). Most of the package isn't anything special; it is a basic wrapper around the aforemention StatsCan API. The interesting part is how we're handling <get_table_from_name(arg)> method. This too is quite simple; we send a basic get request to StatCan's website under the search tab using the arg. Then, using regular expression, the method parses the HTML table returned by the GET request to construct a 2D array of table names and IDs. Assuming at least one match is found, the method then uses the 0th positioned table_id to query the API. If no matches are found the method returns an exception. There is an additional method to list datasets based off a table name. Similar to the previous method, this one leverages StatsCan's search feature, and returns a parsed list of table names and their assoicated table IDs. This is useful if you don't know exactly which dataset you want to query.",
            "Deploying the package to the Python Package Index (PyPI) was delightfully easy. The bulk of work is handled by the *setuptools* module. which is where you define your package's metadata. Once that is done, the final step is creating your deployment pipeline. For *StatsCanPy*, I used github actions for CI/CD. The deployment pipeline installs the required dependecies, runs unit tests and, assuming they pass, builds the package for deployment. The total runtime of the pipeline is <2 minutes. Once the pipeline run is complete, the package can be installed using *pip*.",
            "All in, I was able to build and deploy v.1 of * StatsCanPy * in ~4 hours. Circling back to my orginal point about ease of use, by the afternoon I was able to begin my analysis -- importing my own module from pip (pretty cool IMO).",
        ]
    },
    {
        "id": "graphs",
        "title": "A Practical Implementation of Graphs",
        "author": "Rielly H. Young",
        "date": "July 22nd, 2024",
        "content": 
        [
            "A common difficulty faced by organizations is the inability to quantify the relationship between abstract business objects. A makeshift solution is often the establishment of key performance indicators (KPIs) which, when aggregated, seek to provide quantifiable metrics related to a specific indicator. These KPIs can be used as proxy to compare various abstract objects against one another. But what if you were trying to compare a large number of products to one another, based off qualities that were unable to be easily quantified? What if your company provides a wide number of services across departments, and you wanted to determine which of these had the most overlap?",
            "This was the problem facing my team when trying to quantify the similarity of programs offered by the organization. We had an array of over 400 programs with 30+ properties of various datatypes that needed to be compared to one another. The goal of this comparison was to identify the programs with the highest similarity – as to allow for more efficient direction of organizational resources. Our solution was to build a graph of program objects for further analysis. The following case will implement the same logic to a different set of abstract objects to demonstrate the usefulness of graphs.",
            "Consider a problem where we need to compare companies listed in the S&P500; trying to find the companies that are the most 'alike'. There are many ways one could define 'alike': the products they sell or the services they engage in, the number of employees they have or their market capitalization. All these methods would arguably work fine, but most likely fail to yield any interesting patterns; I would argue, because they are too narrow in scope. Therein lies the problem: how can we quantify a relationship between two or more objects based on the widest variety of their shared properties? This is where the concept of a graph comes into play.",
            "This by no means is an exhaustive explanation of graphs; rather a primer for the purposes of understanding this example. At the most basic level, a graph is an abstract data structure that is made up of a set of nodes (also called vertices) and a set of unordered edges, which represent a connection between two nodes. These can be directed or undirected, but for our purposes we care about directed. These will sometimes be referred to as digraphs. Edges can also have values, which will be important to our implementation of them. Another import aspect of graphs is their means for traversal. While graph traversal can be accomplished with breadth-first search (BFS) or depth-first search (DFS), our implementation utilized the latter. The importance of this will be highlighted in further detail.",
            "So, we understand now that we’re going to build a graph where the nodes represent stocks in the S&P500 - we’re half of the way there. What is missing? The edges. These represent the distance (or similarity in our case) between two nodes. To calculate this distance between two nodes, we implement a summation of each property’s Levenshtein distance. The Levenshtein distance between two strings is the representation of the number of single character edits required to turn one string into another. Our implementation assumed two strings would have a distance between zero (completely different) to one hundred (identical strings). This is an inverse of a typical implementation, but it will make sense as we move on.",
            "Now we have everything we need to make the comparisons: we know what data structure we’re going to use, and we know how we’re going to quantify the edge distances. The rest is quite simple. Below is the C# implementation of a graph, with a few (not all) methods. Full definitions for Obj and NodePair can be found at the end of this document in Appendix A.",
            "* \npublic class Graph \n{\n   private HashSet<Obj> nodes { get; } = new();\n   private HashSet<NodePair> edges { get; } = new();\n}\n\n",
            "While a typical implementation of the graph data structure has more methods that are shown here, only AddNode and AddEdge are important to us. We start out with a graph with no nodes; we iterate through our list of stocks, adding them to the graph one by one. If nodes are empty, we add a new node; if not, we add an edge to edges between the new node and every other node within nodes. Because our graph’s nodes are bidirectional, we create the minimum number of edges required. The code would look something like this:",
            "* \npublic void Graph.AddNode(Obj node)\n{\n    if (nodes.Count == 0)\n    {\n        nodes.Add(node);\n    }\n    else\n    {\n        foreach (Obj oldNode in nodes)\n        {\n            AddEdge(node, oldNode);\n        }\n        nodes.Add(node);\n    }\n}\n\nprivate void Graph.AddEdge(Obj node1, Obj node2)\n{\n    NodePair np = new(node1, node2);\n    edges.Add(np);\n}\n\n",
            "Distance of the edge is calculated at the edge level (again, which can be seen in Appendix A). Using object reflection, we compare every property of the object (stock in our case). Once the nodes have been added to the graph, determining which nodes are most alike is as simple as providing the GetEdges(double threshold) method a threshold.",
            "For example, if the threshold was set to 85, only edges with values greater than 85 would be returned - in other words, only stock pairs that were more than 85% identical would be shown. Thus, we now have a list containing the most similar stocks in descending order that we can use for further analysis. ",
            "* \npublic List<NodePair> Graph.GetEdges(double threshold)\n{\n     return edges.Where(edge => threshold <= edge.distance).ToList().OrderByDecending(e => e.distance).ToList();\n}\n\n",
            "There are five hundred stocks in the S&P500. If we wanted an algorithm to compare each stock to every other one, it’s algorithmic complexity would be O(n!) where n! represents a factorial of the number of stocks (500!). On the other hand, implementing a BFS of our stock graph would have a time complexity of O(|N| + |E|), where N represents the number of nodes (stocks) and E represents the number of edge pairs. From an efficiency perspective, the implementation of a graph based DFS is by far the superior method. ",
            "Wrapping up, the implementation of a graph with edge distances calculated by Levenshtein distance is an easy & efficient means for quantifying the relationship between abstract objects. My initial implementation for determining the similarities between programs across our organization. We were able to make connections between programs that hadn’t been previous identified due to the vast quantity of data.",
            "Appendix A.",
            "* \nusing FuzzySharp; \n\npublic class Obj\n{\n    public string prop1 { get; set; }\n    public int prop2 { get; set;}\n    public DateTime prop3 { get; set;}\n    \n    // remaining properties\n}\n\npublic class NodePair\n{\n    public Obj node1 { get; set; }\n    public Obj node2 { get; set; }\n    public int distance { get; }\n\n    public NodePair( Obj one, Obj two)\n    {\n        node1 = one;\n        node2 = two;\n        distance = SetEdgeDistance(one, two);\n    }\n\n    private static double SetEdgeDistance(Obj node1, Obj node2)\n    {\n        PropertyInfo[] properties = typeof(Obj).GetProperties();\n        int numberOfProperties = properties.Length;\n        double distance = 0;\n        for (int i = 0; i < numberOfProperties; i++)\n        {\n            object val1 = properties[i].GetValue(node1);\n            object val2 = properties[i].GetValue(node2);\n            if (val1 == null || val2 == null)\n            {\n                distance = 1; // when both props are null it is treated as identical.\n            }\n            else\n            {\n                distance = Levenshtein.GetRatio(val1.ToString(), val2.ToString());\n            }\n        }\n        return distance/numberOfProperties;\n    }\n}"
        ]
    }
]
